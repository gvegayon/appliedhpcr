```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = FALSE, results = 'asis', warning = FALSE, comment = "")
cat2 <- function(..., lang = 'r') {
  cat("```", lang, "\n", sep = "")
  cat(...)
  cat("```\n")
}
```


# A brief intro to Slurm

For a quick-n-dirty intro to Slurm [@Yoo_Jette_Grondona_2003], we will start with a simple "Hello world"
using Slurm + R. For this, we need to go through the following steps:

1.  Copy a Slurm script to HPC,

2.  Logging to HPC, and

3.  Submit the job using `sbatch`.


## Step 1: Copy the Slurm script to HPC

We need to copy the following Slurm script to HPC ([00-hello-world.slurm](00-hello-world.slurm){target="_blank"}):

```{r}
#| label: helloworld
cat2(readLines("00-hello-world.slurm"), sep = "\n", lang = "bash")
```

Which has four lines:

1.  `#!/bin/sh`: The **shebang** ([**shewhat?**](https://stackoverflow.com/questions/7366775/what-does-the-line-bin-sh-mean-in-a-unix-shell-script))

2.  `#SBATCH --output=00-hello-world.out`: An option to be passed to `sbatch`, in
    this case, the name of the output file to which
    [**stdout and stderr**](https://en.wikipedia.org/wiki/Standard_streams) will go.
    
3.  `module load R/4.2.2`: Uses [**Lmod**](https://lmod.readthedocs.io/en/latest/) to load the `R`
    module.
    
4.  `Rscript ...`: A call to R to evaluate the expression `paste(...)`. This will
    get the environment variable `SLURMD_NODENAME` (which `sbatch` creates) and
    print it on a message.


To do so, we will use **Secure copy protocol (scp)**, which allows us to copy
data to and fro computers. In this case, we should do something like the following

```bash
scp 00-hello-world.slurm [userid]@notchpeack.chpc.utah.edu:/path/to/a/place/you/can/access
```

In words, "Using the username `[userid]`, connect to `notchpeack.chpc.utah.edu`, take
the file `00-hello-world.slurm` and copy it to `/path/to/a/place/you/can/access`. With the file
now available in the cluster, we can submit this job using Slurm.


## Step 2: Logging to HPC

1.  Log in using ssh. In the case of Windows users, download the
    [**Putty**](https://www.chiark.greenend.org.uk/~sgtatham/putty/) client.

2.  To log in, you will need to use your organization ID. Usually, if your email is something like
    `myemailuser@school.edu`, your ID is `myemailuser`. Then:
    
    ``` {.bash}
    ssh myemailuser@notchpeack.chpc.utah.edu
    ```


## Step 3: Submitting the job

Overall, there are two ways to use the compute nodes: interactively (`salloc`)
and in batch mode (`sbatch`).  In this case, since we have a Slurm script, we
will use the latter.

To submit the job, we can type the following:

```bash
sbatch 00-hello-world.slurm
```

And that's it!

In the case of interactive sessions, You can start one using the `salloc` command.
For example, if you wanted to run R with 8 cores, using 16 Gigs of memory in
total, you would need to do the following:
    
```bash
salloc -n1 --cpus-per-task=8 --mem-per-cpu=2G --time=01:00:00
```

Once your request is submitted, you will get access to a compute node.
Within it, you can load the required modules and start R:

```bash
module load R/4.2.2
R
```

Interactive sessions are not recommended for long jobs. Instead, use this
resource if you need to inspect some large dataset, debug your code, etc.
