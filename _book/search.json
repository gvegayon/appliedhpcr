[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied HPC with R",
    "section": "",
    "text": "Preface\nThe R programming language (R Core Team 2023) can be amazing for most daily tasks. But as soon as you start dealing with more complicated problems, you may face the for-loop bottle-neck. If you ever encounter such a problem, then this book is for you. Applied HPC with R is a collection of talks and lectures I have given in the past about speeding up your R code using parallel computing and other resources such as C++. The contents have been mostly developed during my time at USC and UofU1"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Applied HPC with R",
    "section": "",
    "text": "With many to thank, including Paul Marjoram, Zhi Zhang, Emil Hvitfeldt, Malcolm Barrett, Garrett Weaver, USC’s IMAGE P01 research group, and my students both at USC and UoU.↩︎"
  },
  {
    "objectID": "intro.html#high-performance-computing-an-overview",
    "href": "intro.html#high-performance-computing-an-overview",
    "title": "1  Introduction",
    "section": "1.1 High-Performance Computing: An overview",
    "text": "1.1 High-Performance Computing: An overview\nLoosely, from R’s perspective, we can think of HPC in terms of two, maybe three things:\n\nBig data: How to work with data that doesn’t fit your computer\nParallel computing: How to take advantage of multiple core systems\nCompiled code: Write your low-level code (if R doesn’t have it yet…)\n\n(Checkout CRAN Task View on HPC)"
  },
  {
    "objectID": "intro.html#big-data",
    "href": "intro.html#big-data",
    "title": "1  Introduction",
    "section": "1.2 Big Data",
    "text": "1.2 Big Data\n\nBuy a bigger computer/RAM (not the best solution!)\nUse out-of-memory storage, i.e., don’t load all your data in the RAM. e.g. The bigmemory, data.table, HadoopStreaming R packages\nEfficient algorithms for big data, e.g.: biglm, biglasso\nStore it more efficiently, e.g.: Sparse Matrices (take a look at the dgCMatrix objects from the Matrix R package)"
  },
  {
    "objectID": "intro.html#parallel-computing",
    "href": "intro.html#parallel-computing",
    "title": "1  Introduction",
    "section": "1.3 Parallel computing",
    "text": "1.3 Parallel computing\n\n\n\n\n\nFlynn’s Classical Taxonomy (Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory)\n\n\n\n\nWe will be focusing on the Single Instruction stream Multiple Data stream"
  },
  {
    "objectID": "intro.html#parallel-computing-1",
    "href": "intro.html#parallel-computing-1",
    "title": "1  Introduction",
    "section": "1.4 Parallel computing",
    "text": "1.4 Parallel computing\nIn general terms, a parallel computing program is one in which we use two or more computational threads simultaneously. Although computational thread usually means core, there are multiple levels at which a computer program can be parallelized. To understand this, we first need to see what composes a modern computer:\n\n\n\nSource: Original figure from LUMI consortium documentation (LUMI consortium 2023)\n\n\nStreaming SIMD Extensions [SSE] and Advanced Vector Extensions [AVX]\n\n1.4.1 Serial vs Parallel\n\n  source: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory"
  },
  {
    "objectID": "intro.html#parallel-computing-2",
    "href": "intro.html#parallel-computing-2",
    "title": "1  Introduction",
    "section": "1.5 Parallel computing",
    "text": "1.5 Parallel computing\n\n\n\n\n\nsource: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory"
  },
  {
    "objectID": "intro.html#some-vocabulary-for-hpc",
    "href": "intro.html#some-vocabulary-for-hpc",
    "title": "1  Introduction",
    "section": "1.6 Some vocabulary for HPC",
    "text": "1.6 Some vocabulary for HPC\nIn raw terms\n\nSupercomputer: A single big machine with thousands of cores/GPGPUs.\nHigh-Performance Computing (HPC): Multiple machines within a single network.\nHigh Throughput Computing (HTC): Multiple machines across multiple networks.\n\nYou may not have access to a supercomputer, but certainly, HPC/HTC clusters are more accessible these days, e.g. AWS provides a service to create HPC clusters at a low cost (allegedly, since nobody understands how pricing works)"
  },
  {
    "objectID": "intro.html#gpu-vs-cpu",
    "href": "intro.html#gpu-vs-cpu",
    "title": "1  Introduction",
    "section": "1.7 GPU vs CPU",
    "text": "1.7 GPU vs CPU\n\n\n\n\n\nNVIDIA Blog\n\n\n\n\n\nWhy use OpenMP if GPU is suited to compute-intensive operations? Well, mostly because OpenMP is VERY easy to implement (easier than CUDA, which is the easiest way to use GPU).2"
  },
  {
    "objectID": "intro.html#when-is-it-a-good-idea",
    "href": "intro.html#when-is-it-a-good-idea",
    "title": "1  Introduction",
    "section": "1.8 When is it a good idea?",
    "text": "1.8 When is it a good idea?\n\n\n\n\n\nAsk yourself these questions before jumping into HPC!"
  },
  {
    "objectID": "intro.html#parallel-computing-in-r",
    "href": "intro.html#parallel-computing-in-r",
    "title": "1  Introduction",
    "section": "1.9 Parallel computing in R",
    "text": "1.9 Parallel computing in R\nWhile there are several alternatives (just take a look at the High-Performance Computing Task View), we’ll focus on the following R-packages for explicit parallelism:\n\nparallel: R package that provides ‘[s]upport for parallel computation, including random-number generation’.\nfuture: ‘[A] lightweight and unified Future API for sequential and parallel processing of R expression via futures.’\nRcpp + OpenMP: Rcpp is an R package for integrating R with C++ and OpenMP is a library for high-level parallelism for C/C++ and FORTRAN.\n\n\nOthers but not used here\n\nforeach for iterating through lists in parallel.\nRmpi for creating MPI clusters.\n\nAnd tools for implicit parallelism (out-of-the-box tools that allow the programmer not to worry about parallelization):\n\ngpuR for Matrix manipulation using GPU\ntensorflow an R interface to TensorFlow.\n\nA ton of other types of resources, notably the tools for working with batch schedulers such as Slurm, and HTCondor.\n\n\n\n\nDowle, Matt, and Arun Srinivasan. 2021. Data.table: Extension of ‘Data.frame‘. https://CRAN.R-project.org/package=data.table.\n\n\nLUMI consortium. 2023. “Documentation - Distribution and Binding.” https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/distribution-binding/."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Nonetheless this claim can be said about almost any programming language, there are notable like the R package data.table (Dowle and Srinivasan 2021) which has been demonstrated to out-perform most data wrangling tools.↩︎\nSadia National Laboratories started the Kokkos project, which provides a one-fits-all C++ library for parallel programming. More information on the Kokkos’s wiki site.↩︎"
  },
  {
    "objectID": "parallel-pkg.html#parallel-workflow",
    "href": "parallel-pkg.html#parallel-workflow",
    "title": "2  The parallel R package",
    "section": "2.1 Parallel workflow",
    "text": "2.1 Parallel workflow\n(Usually) We do the following:\n\nCreate a PSOCK/FORK (or other) cluster using makePSOCKCluster/makeForkCluster (or makeCluster)\nCopy/prepare each R session (if you are using a PSOCK cluster):\n\nCopy objects with clusterExport\nPass expressions with clusterEvalQ\nSet a seed\n\nDo your call: parApply, parLapply, etc.\nStop the cluster with clusterStop"
  },
  {
    "objectID": "parallel-pkg.html#types-of-clusters-psock",
    "href": "parallel-pkg.html#types-of-clusters-psock",
    "title": "2  The parallel R package",
    "section": "2.2 Types of clusters: PSOCK",
    "text": "2.2 Types of clusters: PSOCK\n\nCan be created with makePSOCKCluster\nCreates brand new R Sessions (so nothing is inherited from the master), e.g.\n# This creates a cluster with 4 R sessions\ncl &lt;- makePSOCKCluster(4)\nChild sessions are connected to the master session via Socket connections\nCan be created outside of the current computer, i.e. across multiple computers!"
  },
  {
    "objectID": "parallel-pkg.html#types-of-clusters-fork",
    "href": "parallel-pkg.html#types-of-clusters-fork",
    "title": "2  The parallel R package",
    "section": "2.3 Types of clusters: Fork",
    "text": "2.3 Types of clusters: Fork\n\nFork Cluster makeForkCluster:\nUses OS Forking,\nCopies the current R session locally (so everything is inherited from the master up to that point).\nData is only duplicated if it is altered (need to double check when this happens!)\nNot available on Windows.\n\nOther makeCluster: passed to snow (Simple Network of Workstations)"
  },
  {
    "objectID": "parallel-pkg.html#ex-1-parallel-rng-with-makepsockcluster",
    "href": "parallel-pkg.html#ex-1-parallel-rng-with-makepsockcluster",
    "title": "2  The parallel R package",
    "section": "2.4 Ex 1: Parallel RNG with makePSOCKCluster",
    "text": "2.4 Ex 1: Parallel RNG with makePSOCKCluster\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\nnnodes &lt;- 4L\ncl     &lt;- makePSOCKcluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`\n# 3. DO YOUR CALL\nans &lt;- parSapply(cl, 1:nnodes, function(x) runif(1e3))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\n\nMaking sure is reproducible\n\n# I want to get the same!\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(parSapply(cl, 1:nnodes, function(x) runif(1e3)))\n# 4. STOP THE CLUSTER\nstopCluster(cl)\nall.equal(ans0, ans1) # All equal!\n\n[1] TRUE"
  },
  {
    "objectID": "parallel-pkg.html#ex-2-parallel-rng-with-makeforkcluster",
    "href": "parallel-pkg.html#ex-2-parallel-rng-with-makeforkcluster",
    "title": "2  The parallel R package",
    "section": "2.5 Ex 2: Parallel RNG with makeForkCluster",
    "text": "2.5 Ex 2: Parallel RNG with makeForkCluster\nIn the case of makeForkCluster\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\ncl     &lt;- makeForkCluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123)\n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, parLapply(cl, 1:nnodes, function(x) {\n  runif(nsims) # Look! we use the nsims object!\n               # This would have fail in makePSOCKCluster\n               # if we didn't copy -nsims- first.\n  }))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\n\nAgain, we want to make sure this is reproducible\n\n# Same sequence with same seed\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(do.call(cbind, parLapply(cl, 1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\n# 4. STOP THE CLUSTER\nstopCluster(cl)\n\nWell, if you are a Mac-OS/Linux user, there’s a simpler way of doing this…"
  },
  {
    "objectID": "parallel-pkg.html#ex-3-parallel-rng-with-mclapply-forking-on-the-fly",
    "href": "parallel-pkg.html#ex-3-parallel-rng-with-mclapply-forking-on-the-fly",
    "title": "2  The parallel R package",
    "section": "2.6 Ex 3: Parallel RNG with mclapply (Forking on the fly)",
    "text": "2.6 Ex 3: Parallel RNG with mclapply (Forking on the fly)\nIn the case of mclapply, the forking (cluster creation) is done on the fly!\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\n# cl     &lt;- makeForkCluster(nnodes) # mclapply does it on the fly\n# 2. PREPARING THE CLUSTER\nset.seed(123) \n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims)))\n(ans0 &lt;- var(ans))\n\n             [,1]        [,2]         [,3]         [,4]\n[1,]  0.085384184 0.002390790  0.006576204 -0.003998278\n[2,]  0.002390790 0.081142190  0.001846963  0.001476244\n[3,]  0.006576204 0.001846963  0.085175347 -0.002807348\n[4,] -0.003998278 0.001476244 -0.002807348  0.082425477\n\n\n\nOnce more, we want to make sure this is reproducible\n\n# Same sequence with same seed\nset.seed(123) \nans1 &lt;- var(do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\n# 4. STOP THE CLUSTER\n# stopCluster(cl) no need of doing this anymore"
  },
  {
    "objectID": "slurm-fundamentals.html",
    "href": "slurm-fundamentals.html",
    "title": "3  What is Slurm",
    "section": "",
    "text": "4 A brief intro to Slurm\nFor a quick-n-dirty intro to Slurm (Yoo, Jette, and Grondona 2003), we will start with a simple “Hello world” using Slurm + R. For this, we need to go through the following steps:"
  },
  {
    "objectID": "slurm-fundamentals.html#definitions",
    "href": "slurm-fundamentals.html#definitions",
    "title": "3  What is Slurm",
    "section": "3.1 Definitions",
    "text": "3.1 Definitions\nFirst, some important discussion points within the context of Slurm+R that users, in general, will find useful. Most of the points have to do with options available for Slurm, and in particular, with the sbatch command with is used to submit batch jobs to Slurm. Users who have used Slurm in the past may wish to skip this and continue reading the following section.\n\nNode A single computer in the HPC: A lot of times jobs will be submitted to a single node. The simplest way of using R+Slurm is submitting a single job and requesting multiple CPUs to use, for example, parallel::parLapply or parallel::mclapply. Usually, users do not need to request a specific number of nodes to be used as Slurm will allocate the resources as needed.\nA common mistake of R users is to specify the number of nodes and expect that their script will be parallelized. This won’t happen unless the user explicitly writes a parallel computing script.\nThe relevant flag for sbatch is --nodes.\nPartition A group of nodes in HPC. Generally large nodes may have multiple partitions, meaning that nodes may be grouped in various ways. For example, nodes belonging to a single group of users may be in a single partition, and nodes dedicated to working with large data may be in another partition. Usually, partitions are associated with account privileges, so users may need to specify which account are they using when telling Slurm what partition they plan to use.\nThe relevant flag for sbatch is --partition.\nAccount Accounts may be associated with partitions. Accounts can have privileges to use a partition or set of nodes. Often, users need to specify the account when submitting jobs to a particular partition.\nThe relevant flag for sbatch is --account.\nTask A step within a job. A particular job can have multiple tasks. tasks may span multiple nodes, so if the user wants to submit a multicore job, this option may not be the right one.\nThe relevant flag for sbatch is --ntasks\nCPU generally this refers to core or thread (which may be different in systems supporting multithreaded cores). Users may want to specify how many CPUs they want to use for a task. And this is the relevant option when using things like OpenMP or functions that allow creating cluster objects in R (e.g. makePSOCKcluster, makeForkCluster).\nThe relevant option in sbatch is --cpus-per-task. More information regarding CPUs in Slurm can be found here. Information regarding how Slurm counts CPUs/cores/threads can be found here.\nJob Array Slurm supports job arrays. A job array is in simple terms a job that is repeated multiple times by Slurm, this is, replicates a single job as requested per the user. In the case of R, when using this option, a single R script is spanned in multiple jobs, so the user can take advantage of this and parallelize jobs across multiple nodes. Besides from the fact that jobs within a Job Array may be spanned across multiple nodes, each job in that array has a unique ID that is available to the user via environment variables, in particular SLURM_ARRAY_TASK_ID.\nWithin R, and hence the Rscript submitted to Slurm, users can access this environment variable with Sys.getenv(\"SLURM_ARRAY_TASK_ID\"). Some of the functionalities of slurmR rely on Job Arrays.\nMore information on Job Arrays can be found here. The relevant option for this in sbatch is --array.\n\nMore information about Slurm can be found their official website here. A tutorial about how to use Slurm with R can be found here."
  },
  {
    "objectID": "slurm-fundamentals.html#step-1-copy-the-slurm-script-to-hpc",
    "href": "slurm-fundamentals.html#step-1-copy-the-slurm-script-to-hpc",
    "title": "3  What is Slurm",
    "section": "4.1 Step 1: Copy the Slurm script to HPC",
    "text": "4.1 Step 1: Copy the Slurm script to HPC\nWe need to copy the following Slurm script to HPC (00-hello-world.slurm):\n#!/bin/sh\n#SBATCH --output=00-hello-world.out\nmodule load R/4.2.2\nRscript -e \"paste('Hello from node', Sys.getenv('SLURMD_NODENAME'))\"\nWhich has four lines:\n\n#!/bin/sh: The shebang (shewhat?)\n#SBATCH --output=00-hello-world.out: An option to be passed to sbatch, in this case, the name of the output file to which stdout and stderr will go.\nmodule load R/4.2.2: Uses Lmod to load the R module.\nRscript ...: A call to R to evaluate the expression paste(...). This will get the environment variable SLURMD_NODENAME (which sbatch creates) and print it on a message.\n\nTo do so, we will use Secure copy protocol (scp), which allows us to copy data to and fro computers. In this case, we should do something like the following\nscp 00-hello-world.slurm [userid]@notchpeak.chpc.utah.edu:/path/to/a/place/you/can/access\nIn words, “Using the username [userid], connect to notchpeak.chpc.utah.edu, take the file 00-hello-world.slurm and copy it to /path/to/a/place/you/can/access. With the file now available in the cluster, we can submit this job using Slurm."
  },
  {
    "objectID": "slurm-fundamentals.html#step-2-logging-to-hpc",
    "href": "slurm-fundamentals.html#step-2-logging-to-hpc",
    "title": "3  What is Slurm",
    "section": "4.2 Step 2: Logging to HPC",
    "text": "4.2 Step 2: Logging to HPC\n\nLog in using ssh. In the case of Windows users, download the Putty client.\nTo log in, you will need to use your organization ID. Usually, if your email is something like myemailuser@school.edu, your ID is myemailuser. Then:\nssh myemailuser@notchpeak.chpc.utah.edu"
  },
  {
    "objectID": "slurm-fundamentals.html#step-3-submitting-the-job",
    "href": "slurm-fundamentals.html#step-3-submitting-the-job",
    "title": "3  What is Slurm",
    "section": "4.3 Step 3: Submitting the job",
    "text": "4.3 Step 3: Submitting the job\nOverall, there are two ways to use the compute nodes: interactively (salloc) and in batch mode (sbatch). In this case, since we have a Slurm script, we will use the latter.\nTo submit the job, we can type the following:\nsbatch 00-hello-world.slurm\nAnd that’s it! That said, it is often required to specify the account and partition the user will be submitting the job. For example, if you have the account my-account and partition my-partition associated with your user, you can incorporate that information as follows:\nsbatch 00-hello-world.slurm --acount=my-account --partition=my-partition\nIn the case of interactive sessions, You can start one using the salloc command. For example, if you wanted to run R with 8 cores, using 16 Gigs of memory in total, you would need to do the following:\nsalloc -n1 --cpus-per-task=8 --mem-per-cpu=2G --time=01:00:00\nOnce your request is submitted, you will get access to a compute node. Within it, you can load the required modules and start R:\nmodule load R/4.2.2\nR\nInteractive sessions are not recommended for long jobs. Instead, use this resource if you need to inspect some large dataset, debug your code, etc.\n\n\n\n\nYoo, Andy B., Morris A. Jette, and Mark Grondona. 2003. “SLURM: Simple Linux Utility for Resource Management.” In Job Scheduling Strategies for Parallel Processing, edited by Dror Feitelson, Larry Rudolph, and Uwe Schwiegelshohn, 2862:44–60. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/10968987_3."
  },
  {
    "objectID": "slurm-simpi.html#submitting-jobs-to-slurm",
    "href": "slurm-simpi.html#submitting-jobs-to-slurm",
    "title": "4  Simulating pi (once more)",
    "section": "4.1 Submitting jobs to Slurm",
    "text": "4.1 Submitting jobs to Slurm\nThe main way that we will be working is by submitting jobs using the sbatch function. This function takes as a main argument a bash file with the program to execute. In the case of R, a regular bash file looks something like this:\n#!/bin/sh\n#SBATCH --job-name=sapply\n#SBATCH --time=00:10:00\n\nmodule load R/4.2.2\nRscript --vanilla 01-sapply.R\nThis file has three components:\n\nThe Slurm flags #SBATCH: These comment-like entries are used to pass Slurm options to the job. In this example, we are only specifying the options job-name and time. Other common options to include would be account and partition.\nLoading R module load R/4.2.2: Depending on your system’s configuration, you may or may not need to load modules or run bash scripts before being able to run R. In this example, we are loading R version 4.2.2 using LMod (see previous section).\nExecuting the R script: Finally, after specifying Slurm options and loading whatever needs to be loaded before executing R, we are using RScript to execute the program we wrote.\n\nSubmission is then done as follows:\nsbatch 01-sapply.slurm\nThe following examples have two files, a bash script and an R script to be called by Slurm.\n\n4.1.1 Case 1: Single job, single core job\nThe most basic way is submitting a job using the sbatch command. In this case, you need to have 2 files: (1) An R script, and (2) a bash script. e.g.\nThe contents of the R script (01-sapply.R) are:\n# Model parameters\nnsims &lt;- 1e3\nn     &lt;- 1e4\n\n# Function to simulate pi\nsimpi &lt;- function(i) {\n  \n  p &lt;- matrix(runif(n*2, -1, 1), ncol = 2)\n  mean(sqrt(rowSums(p^2)) &lt;= 1) * 4\n  \n}\n\n# Approximation\nset.seed(12322)\nans &lt;- sapply(1:nsims, simpi)\n\nmessage(\"Pi: \", mean(ans))\n\nsaveRDS(ans, \"01-sapply.rds\")\nThe contents of the bashfile (01-sapply.slurm) are:\n#!/bin/sh\n#SBATCH --job-name=sapply\n#SBATCH --time=00:10:00\n\nmodule load R/4.2.2\nRscript --vanilla 01-sapply.R\n\n\n4.1.2 Case 2: Single job, multicore job\nNow, imagine that we would like to use more than one processor for this job, using something like the parallel::mclapply function from the parallel package.1 Then, besides adapting the code, we need to tell Slurm that we are using more than one core per task, as in the following example:\nR script (02-mclapply.R):\n# Model parameters\nnsims  &lt;- 1e3\nn      &lt;- 1e4\nncores &lt;- 4L\n\n# Function to simulate pi\nsimpi &lt;- function(i) {\n  \n  p &lt;- matrix(runif(n*2, -1, 1), ncol = 2)\n  mean(sqrt(rowSums(p^2)) &lt;= 1) * 4\n  \n}\n\n# Approximation\nset.seed(12322)\nans &lt;- parallel::mclapply(1:nsims, simpi, mc.cores = ncores)\nans &lt;- unlist(ans)\n\nmessage(\"Pi: \", mean(ans))\n\nsaveRDS(ans, \"02-mclpply.rds\")\nBashfile (02-mclapply.slurm):\n#!/bin/sh\n#SBATCH --job-name=mclapply\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=4\n\nmodule load R/4.2.2\nRscript --vanilla 02-mclapply.R"
  },
  {
    "objectID": "slurm-simpi.html#jobs-with-the-slurmr-package",
    "href": "slurm-simpi.html#jobs-with-the-slurmr-package",
    "title": "4  Simulating pi (once more)",
    "section": "4.2 Jobs with the slurmR package",
    "text": "4.2 Jobs with the slurmR package\nThe slurmR R package (Vega Yon and Marjoram 2019, 2022) is a lightweight wrapper of Slurm. The package’s main functions are the *apply family–mostly through Slurm job arrays–and the makeSlurmCluster()–which is a wrapper of makePSOCKcluster.\nThis section will illustrate how to submit jobs using the makeSlurmCluster() function and Slurm_sapply. Furthermore, the last example demonstrates how we can skip writing Slurm scripts entirely using the sourceSlurm() function included in the package.\n\n4.2.1 Case 3: Single job, multinode job\nIn this case, there is no simple way to submit a multinodal job to Slurm… unless you use the slurmR package.2 In this example, we will combine slurmR with the parallel package’s parSapply function to submit a multinodal job using the function makeSlurmCluster(). With it, slurmR will submit a job requesting njobs tasks (processors) that could span multiple nodes,3 and create a Socket cluster out of it (like using makePSOCKcluster.) One thing to keep in mind is that Socket clusters are limited in the number of connections a single R session can span. You can read more about it here and here.\nR script (03-parsapply-slurmr.R):\n# Model parameters\nnsims  &lt;- 1e3\nn      &lt;- 1e4\nncores &lt;- 4L\n\n# Function to simulate pi\nsimpi &lt;- function(i) {\n  \n  p &lt;- matrix(runif(n*2, -1, 1), ncol = 2)\n  mean(sqrt(rowSums(p^2)) &lt;= 1) * 4\n  \n}\n\n# Setting up slurmR\nlibrary(slurmR) # This also loads the parallel package\n\n# Making the cluster, and exporting the variables\ncl &lt;- makeSlurmCluster(ncores)\n\n# Approximation\nclusterExport(cl, c(\"n\", \"simpi\"))\nans &lt;- parSapply(cl, 1:nsims, simpi)\n\n# Closing connection\nstopCluster(cl)\n\nmessage(\"Pi: \", mean(ans))\n\nsaveRDS(ans, \"03-parsapply-slurmr.rds\")\nBashfile (03-parsapply-slurmr.slurm):\n#!/bin/sh\n#SBATCH --job-name=parsapply\n#SBATCH --time=00:10:00\n\nmodule load R/4.2.2\nRscript --vanilla 03-parsapply-slurmr.R\n\n\n4.2.2 Case 4: Multi job, single/multi-core\nAnother way to submit jobs is using job arrays. A job array is essentially a job that is repeated njobs times with the same configuration. The main difference between replicates is what you do with the SLURM_ARRAY_TASK_ID environment variable. This variable is defined within each replicate and can be used to make the “subjob” depending on that.\nHere is a quick example using R\nID &lt;- Sys.getenv(\"SLURM_ARRAY_TASK_ID\")\nif (ID == 1) {\n  ...[do this]...\n} else if (ID == 2) {\n  ...[do that]...\n}\nThe slurmR R package makes submitting job arrays easy. Again, with the simulation of pi, we can do it in the following way:\nR script (04-slurm_sapply.R):\n# Model parameters\nnsims  &lt;- 1e3\nn      &lt;- 1e4\n# ncores &lt;- 4L\nnjobs  &lt;- 4L\n\n# Function to simulate pi\nsimpi &lt;- function(i, n.) {\n  \n  p &lt;- matrix(runif(n.*2, -1, 1), ncol = 2)\n  mean(sqrt(rowSums(p^2)) &lt;= 1) * 4\n  \n}\n\n# Setting up slurmR\nlibrary(slurmR) # This also loads the parallel package\n\n# Approximation\nans &lt;- Slurm_sapply(\n  1:nsims, simpi,\n  n.       = n,\n  njobs    = njobs,\n  plan     = \"collect\",\n  tmp_path = \"/scratch/vegayon\" # This is where all temp files will be exported\n  )\n\nmessage(\"Pi: \", mean(ans))\n\nsaveRDS(ans, \"04-slurm_sapply.rds\")\nBashfile (04-slurm_sapply.slurm):\n#!/bin/sh\n#SBATCH --job-name=slurm_sapply\n#SBATCH --time=00:10:00\n\nmodule load R/4.2.2\nRscript --vanilla 04-slurm_sapply.R\nOne of the main benefits of using this approach instead of the makeSlurmCluster function (and thus, working with a SOCK cluster) are:\n\nThe number of jobs is not limited here (only by the admin, but not by R).\nIf a job fails, then we can re-run it using sbatch once again (see example here).\nYou can check the individual logs of each process using the function Slurm_lob().\nYou can submit the job and quit the R session without waiting for it to finalize. You can always read back the job using the function read_slurm_job([path-to-the-temp])\n\n\n\n4.2.3 Case 5: Skipping the .slurm file\nThe slurmR package has a function named sourceSlurm that can be used to avoid creating the .slurm file. The user can add the SBATCH options to the top of the R script (including the #!/bin/sh line) and submit the job from within R as follows:\nR script (05-sapply.R):\n#!/bin/sh\n#SBATCH --job-name=sapply-sourceSlurm\n#SBATCH --time=00:10:00\n\n# Model parameters\nnsims &lt;- 1e3\nn     &lt;- 1e4\n\n# Function to simulate pi\nsimpi &lt;- function(i) {\n  \n  p &lt;- matrix(runif(n*2, -1, 1), ncol = 2)\n  mean(sqrt(rowSums(p^2)) &lt;= 1) * 4\n  \n}\n\n# Approximation\nset.seed(12322)\nans &lt;- sapply(1:nsims, simpi)\n\nmessage(\"Pi: \", mean(ans))\n\nsaveRDS(ans, \"05-sapply.rds\")\nFrom the R console (is OK if you are in the Head node)\nslurmR::sourceSlurm(\"05-sapply.R\")\nAnd voilá! A temporary bash file will be generated and used to submit the R script to the queue.\n\n\n\n\nVega Yon, George, and Paul Marjoram. 2019. “slurmR: A Lightweight Wrapper for HPC with Slurm.” The Journal of Open Source Software 4 (39). https://doi.org/10.21105/joss.01493.\n\n\n———. 2022. slurmR: A Lightweight Wrapper for ’Slurm’. https://github.com/USCbiostats/slurmR."
  },
  {
    "objectID": "slurm-simpi.html#footnotes",
    "href": "slurm-simpi.html#footnotes",
    "title": "4  Simulating pi (once more)",
    "section": "",
    "text": "This function is sort of a wrapper of makeForkcluster. Forking provides a way to duplicate a process in the OS without replicating the memory, which is both faster and efficient.↩︎\nSee installation instructions here↩︎\nAlthough possible, most multinode jobs will be allocated only if there are not enough threads within a single node. Remember Slurm does not run the jobs, but rather reserves computational resources for you to run it.↩︎"
  },
  {
    "objectID": "rcpp-part1.html#before-we-start",
    "href": "rcpp-part1.html#before-we-start",
    "title": "5  Rcpp",
    "section": "5.1 Before we start",
    "text": "5.1 Before we start\n\n \n\n\nYou need to have Rcpp installed in your system:\ninstall.packages(\"Rcpp\")\nYou need to have a compiler\n\nWindows: You can download Rtools from here.\nMacOS: It is a bit complicated… Here are some options:\n\nCRAN’s manual to get the clang, clang++, and gfortran compilers here.\nA great guide by the coatless professor here\n\n\n\nAnd that’s it!"
  },
  {
    "objectID": "rcpp-part1.html#r-is-great-but",
    "href": "rcpp-part1.html#r-is-great-but",
    "title": "5  Rcpp",
    "section": "5.2 R is great, but…",
    "text": "5.2 R is great, but…\n\nThe problem:\n\nAs we saw, R is very fast… once vectorized\nWhat to do if your model cannot be vectorized?\n\nThe solution: Use C/C++/Fotran! It works with R!\nThe problem to the solution: What R user knows any of those!?\nR has had an API (application programming interface) for integrating C/C++ code with R for a long time.\nUnfortunately, it is not very straightforward"
  },
  {
    "objectID": "rcpp-part1.html#enter-rcpp",
    "href": "rcpp-part1.html#enter-rcpp",
    "title": "5  Rcpp",
    "section": "5.3 Enter Rcpp",
    "text": "5.3 Enter Rcpp\n\nOne of the most important R packages on CRAN.\nAs of January 22, 2023, about 50% of CRAN packages depend on it (directly or not).\nFrom the package description:\n\n\nThe ‘Rcpp’ package provides R functions as well as C++ classes which offer a seamless integration of R and C++"
  },
  {
    "objectID": "rcpp-part1.html#why-bother",
    "href": "rcpp-part1.html#why-bother",
    "title": "5  Rcpp",
    "section": "5.4 Why bother?",
    "text": "5.4 Why bother?\n\nTo draw ten numbers from a normal distribution with sd = 100.0 using R C API:\nSEXP stats = PROTECT(R_FindNamespace(mkString(\"stats\")));\nSEXP rnorm = PROTECT(findVarInFrame(stats, install(\"rnorm\")));\nSEXP call = PROTECT(\n  LCONS( rnorm, CONS(ScalarInteger(10), CONS(ScalarReal(100.0),\n  R_NilValue))));\nSET_TAG(CDDR(call),install(\"sd\"));\nSEXP res = PROTECT(eval(call, R_GlobalEnv));\nUNPROTECT(4);\nreturn res;\nUsing Rcpp:\nEnvironment stats(\"package:stats\");\nFunction rnorm = stats[\"rnorm\"];\nreturn rnorm(10, Named(\"sd\", 100.0));"
  },
  {
    "objectID": "rcpp-part1.html#example-1-looping-over-a-vector",
    "href": "rcpp-part1.html#example-1-looping-over-a-vector",
    "title": "5  Rcpp",
    "section": "5.5 Example 1: Looping over a vector",
    "text": "5.5 Example 1: Looping over a vector\n\n#include&lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector add1(NumericVector x) {\n  NumericVector ans(x.size());\n  for (int i = 0; i &lt; x.size(); ++i)\n    ans[i] = x[i] + 1;\n  return ans;\n}\n\n\nadd1(1:10)\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nMake it sweeter by adding some “sugar” (the Rcpp kind)\n\n#include&lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector add1Cpp(NumericVector x) {\n  return x + 1;\n}\n\n\nadd1Cpp(1:10)\n\n [1]  2  3  4  5  6  7  8  9 10 11"
  },
  {
    "objectID": "rcpp-part1.html#how-much-fast",
    "href": "rcpp-part1.html#how-much-fast",
    "title": "5  Rcpp",
    "section": "5.6 How much fast?",
    "text": "5.6 How much fast?\nCompared to this:\n\nadd1R &lt;- function(x) {\n  for (i in 1:length(x))\n    x[i] &lt;- x[i] + 1\n  x\n}\nmicrobenchmark::microbenchmark(add1R(1:1000), add1Cpp(1:1000))\n\nUnit: microseconds\n            expr    min     lq     mean  median      uq      max neval cld\n   add1R(1:1000) 33.979 34.492 51.45831 35.8840 36.4935 1579.275   100  a \n add1Cpp(1:1000)  2.260  2.535  8.13880  2.9835  5.4980  422.496   100   b"
  },
  {
    "objectID": "rcpp-part1.html#main-differences-between-r-and-c",
    "href": "rcpp-part1.html#main-differences-between-r-and-c",
    "title": "5  Rcpp",
    "section": "5.7 Main differences between R and C++",
    "text": "5.7 Main differences between R and C++\n\nOne is compiled, and the other interpreted\nIndexing objects: In C++ the indices range from 0 to (n - 1), whereas in R is from 1 to n.\nAll expressions end with a ; (optional in R).\nIn C++ object need to be declared, in R not (dynamic)."
  },
  {
    "objectID": "rcpp-part1.html#crcpp-fundamentals-types",
    "href": "rcpp-part1.html#crcpp-fundamentals-types",
    "title": "5  Rcpp",
    "section": "5.8 C++/Rcpp fundamentals: Types",
    "text": "5.8 C++/Rcpp fundamentals: Types\nBesides C-like data types (double, int, char, and bool), we can use the following types of objects with Rcpp:\n\nMatrices: NumericMatrix, IntegerMatrix, LogicalMatrix, CharacterMatrix\nVectors: NumericVector, IntegerVector, LogicalVector, CharacterVector\nAnd more!: DataFrame, List, Function, Environment"
  },
  {
    "objectID": "rcpp-part1.html#parts-of-an-rcpp-program",
    "href": "rcpp-part1.html#parts-of-an-rcpp-program",
    "title": "5  Rcpp",
    "section": "5.9 Parts of “an Rcpp program”",
    "text": "5.9 Parts of “an Rcpp program”\n\n#include&lt;Rcpp.h&gt;\nusing namespace Rcpp\n// [[Rcpp::export]]\nNumericVector add1(NumericVector x) {\n  NumericVector ans(x.size());\n  for (int i = 0; i &lt; x.size(); ++i)\n    ans[i] = x[i] + 1;\n  return ans;\n}\n\nLine by line, we see the following:\n\nThe #include&lt;Rcpp.h&gt; is similar to library(...) in R, it brings in all that we need to write C++ code for Rcpp.\nusing namespace Rcpp is somewhat similar to detach(...). This simplifies syntax. If we don’t include this, all calls to Rcpp members need to be explicit, e.g., instead of typing NumericVector, we would need to type Rcpp::NumericVector\nThe // starts a comment in C++, in this case, the // [[Rcpp::export]] comment is a flag Rcpp uses to “export” this C++ function to R.\nIt is the first part of the function definition. We are creating a function that returns a NumericVector, is called add1, has a single input element named x that is also a NumericVector.\nHere, we are declaring an object called ans, which is a NumericVector with an initial size equal to the size of x. Notice that .size() is called a “member function” of the x object, which is of class NumericVector.\nWe are declaring a for-loop (three parts):\n\nint i = 0 We declare the variable i, an integer, and initialize it at 0.\ni &lt; x.size() This loop will end when i’s value is at or above the length of x.\n++i At each iteration, i will increment in one unit.\n\nans[i] = x[i] + 1 set the i-th element of ans equal to the i-th element of x plus 1.\nreturn ans exists the function returning the vector ans.\n\nNow, where to execute/run this?\n\nYou can use the sourceCpp function from the Rcpp package to run .cpp scripts (this is what I do most of the time).\nThere’s also cppFunction, which allows compiling a single function.\nWrite an R package that works with Rcpp.\n\nFor now, let’s use the first option."
  },
  {
    "objectID": "rcpp-part1.html#example-running-.cpp-file",
    "href": "rcpp-part1.html#example-running-.cpp-file",
    "title": "5  Rcpp",
    "section": "5.10 Example running .cpp file",
    "text": "5.10 Example running .cpp file\nImagine that we have the following file named norm.cpp\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// [[Rcpp::export]]\ndouble normRcpp(NumericVector x) {\n  \n  return sqrt(sum(pow(x, 2.0)));\n  \n}\nWe can compile and obtain this function using this line Rcpp::sourceCpp(\"norm.cpp\"). Once compiled, a function called normRcpp will be available in the current R session."
  },
  {
    "objectID": "rcpp-part1.html#your-turn",
    "href": "rcpp-part1.html#your-turn",
    "title": "5  Rcpp",
    "section": "5.11 Your turn",
    "text": "5.11 Your turn\n\n5.11.1 Problem 1: Adding vectors\n\nUsing what you have just learned about Rcpp, write a function to add two vectors of the same length. Use the following template\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector add_vectors([declare vector 1], [declare vector 2]) {\n  \n  ... magick ...\n  \n  return [something];\n}\n\nNow, we have to check for lengths. Use the stop function to make sure lengths match. Add the following lines in your code\n\nif ([some condition])\n  stop(\"an arbitrary error message :)\");\n\n\n5.11.2 Problem 2: Fibonacci series\n\n\n\n\n\nEach element of the sequence is determined by the following:\n\\[\nF(n) = \\left\\{\\begin{array}{ll}\nn, & \\mbox{ if }n \\leq 1\\\\\nF(n - 1) + F(n - 2), & \\mbox{otherwise}\n\\end{array}\\right.\n\\]\nUsing recursions, we can implement this algorithm in R as follows:\n\nfibR &lt;- function(n) {\n  if (n &lt;= 1)\n    return(n)\n  fibR(n - 1) + fibR(n - 2)\n}\n# Is it working?\nc(\n  fibR(0), fibR(1), fibR(2),\n  fibR(3), fibR(4), fibR(5),\n  fibR(6)\n)\n\n[1] 0 1 1 2 3 5 8\n\n\nNow, let’s translate this code into Rcpp and see how much speed boost we get!\n\n\n5.11.3 Problem 2: Fibonacci series (solution)\n\n\nCode\n#include &lt;Rcpp.h&gt;\n// [[Rcpp::export]]\nint fibCpp(int n) {\n  if (n &lt;= 1)\n    return n;\n  \n  return fibCpp(n - 1) + fibCpp(n - 2);\n  \n}\n\n\n\nmicrobenchmark::microbenchmark(fibR(20), fibCpp(20))\n\nUnit: microseconds\n       expr      min       lq       mean   median       uq      max neval cld\n   fibR(20) 5350.894 5643.050 5982.69115 5724.878 5810.474 8855.542   100  a \n fibCpp(20)   11.928   12.479   24.21755   15.318   21.728  706.360   100   b"
  },
  {
    "objectID": "rcpp-part1.html#rcpparmadillo-and-openmp",
    "href": "rcpp-part1.html#rcpparmadillo-and-openmp",
    "title": "5  Rcpp",
    "section": "5.12 RcppArmadillo and OpenMP",
    "text": "5.12 RcppArmadillo and OpenMP\n\nFriendlier than RcppParallel… at least for ‘I-use-Rcpp-but-don’t-actually-know-much-about-C++’ users (like myself!).\nMust run only ‘Thread-safe’ calls, so calling R within parallel blocks can cause problems (almost all the time).\nUse arma objects, e.g. arma::mat, arma::vec, etc. Or, if you are used to them std::vector objects as these are thread-safe.\nPseudo Random Number Generation is not very straightforward… But C++11 has a nice set of functions that can be used together with OpenMP\nNeed to think about how processors work, cache memory, etc. Otherwise, you could get into trouble… if your code is slower when run in parallel, then you probably are facing false sharing\nIf R crashes… try running R with a debugger (see Section 4.3 in Writing R extensions):\n~$ R --debugger=valgrind\n\n\n5.12.1 RcppArmadillo and OpenMP workflow\n\nTell Rcpp that you need to include that in the compiler:\n#include &lt;omp.h&gt;\n// [[Rcpp::plugins(openmp)]]\nWithin your function, set the number of cores, e.g\n// Setting the cores\nomp_set_num_threads(cores);\nTell the compiler that you’ll be running a block in parallel with OpenMP\n#pragma omp [directives] [options]\n{\n  ...your neat parallel code...\n}\nYou’ll need to specify how OMP should handle the data:\n\nshared: Default, all threads access the same copy.\nprivate: Each thread has its own copy, uninitialized.\nfirstprivate Each thread has its own copy, initialized.\nlastprivate Each thread has its own copy. The last value used is returned.\n\nSetting default(none) is a good practice.\nCompile!\n\n\n\n5.12.2 Ex 5: RcppArmadillo + OpenMP\nOur own version of the dist function… but in parallel!\n\n#include &lt;omp.h&gt;\n#include &lt;RcppArmadillo.h&gt;\n// [[Rcpp::depends(RcppArmadillo)]]\n// [[Rcpp::plugins(openmp)]]\nusing namespace Rcpp;\n// [[Rcpp::export]]\narma::mat dist_par(const arma::mat & X, int cores = 1) {\n  \n  // Some constants\n  int N = (int) X.n_rows;\n  int K = (int) X.n_cols;\n  \n  // Output\n  arma::mat D(N,N);\n  D.zeros(); // Filling with zeros\n  \n  // Setting the cores\n  omp_set_num_threads(cores);\n  \n#pragma omp parallel for shared(D, N, K, X) default(none)\n  for (int i=0; i&lt;N; ++i)\n    for (int j=0; j&lt;i; ++j) {\n      for (int k=0; k&lt;K; k++) \n        D.at(i,j) += pow(X.at(i,k) - X.at(j,k), 2.0);\n      \n      // Computing square root\n      D.at(i,j) = sqrt(D.at(i,j));\n      D.at(j,i) = D.at(i,j);\n    }\n      \n  \n  // My nice distance matrix\n  return D;\n}\n\n\n# Simulating data\nset.seed(1231)\nK &lt;- 5000\nn &lt;- 500\nx &lt;- matrix(rnorm(n*K), ncol=K)\n# Are we getting the same?\ntable(as.matrix(dist(x)) - dist_par(x, 4)) # Only zeros\n\n\n     0 \n250000 \n\n\n\n# Benchmarking!\nmicrobenchmark::microbenchmark(\n  dist(x),                # stats::dist\n  dist_par(x, cores = 1), # 1 core\n  dist_par(x, cores = 2), # 2 cores\n  dist_par(x, cores = 4), # 4 cores\n  times = 1, \n  unit = \"ms\"\n)\n\nUnit: milliseconds\n                   expr      min       lq     mean   median       uq      max\n                dist(x) 2188.525 2188.525 2188.525 2188.525 2188.525 2188.525\n dist_par(x, cores = 1) 2424.293 2424.293 2424.293 2424.293 2424.293 2424.293\n dist_par(x, cores = 2) 1863.913 1863.913 1863.913 1863.913 1863.913 1863.913\n dist_par(x, cores = 4) 1209.131 1209.131 1209.131 1209.131 1209.131 1209.131\n neval\n     1\n     1\n     1\n     1\n\n\n\n\n5.12.3 Ex 6: The future\n\nfuture is an R package that was designed “to provide a very simple and uniform way of evaluating R expressions asynchronously using various resources available to the user.”\nfuture class objects are either resolved or unresolved.\nIf queried, Resolved values are return immediately, and Unresolved values will block the process (i.e. wait) until it is resolved.\nFutures can be parallel/serial, in a single (local or remote) computer, or a cluster of them.\n\nLet’s see a brief example\n\nlibrary(future)\nplan(multicore)\n# We are creating a global variable\na &lt;- 2\n# Creating the futures has only the overhead (setup) time\nsystem.time({\n  x1 %&lt;-% {Sys.sleep(3);a^2}\n  x2 %&lt;-% {Sys.sleep(3);a^3}\n})\n##    user  system elapsed \n##   0.018   0.012   0.030\n# Let's just wait 5 seconds to make sure all the cores have returned\nSys.sleep(3)\nsystem.time({\n  print(x1)\n  print(x2)\n})\n## [1] 4\n## [1] 8\n##    user  system elapsed \n##   0.003   0.000   0.003\n\n\n\n5.12.4 Bonus track 1: Simulating \\(\\pi\\)\n\nWe know that \\(\\pi = \\frac{A}{r^2}\\). We approximate it by randomly adding points \\(x\\) to a square of size 2 centered at the origin.\nSo, we approximate \\(\\pi\\) as \\(\\Pr\\{\\|x\\| \\leq 1\\}\\times 2^2\\)\n\n\n\n\n\n\nThe R code to do this\n\npisim &lt;- function(i, nsim) {  # Notice we don't use the -i-\n  # Random points\n  ans  &lt;- matrix(runif(nsim*2), ncol=2)\n  \n  # Distance to the origin\n  ans  &lt;- sqrt(rowSums(ans^2))\n  \n  # Estimated pi\n  (sum(ans &lt;= 1)*4)/nsim\n}\n\n\nlibrary(parallel)\n# Setup\ncl &lt;- makePSOCKcluster(4L)\nclusterSetRNGStream(cl, 123)\n# Number of simulations we want each time to run\nnsim &lt;- 1e5\n# We need to make -nsim- and -pisim- available to the\n# cluster\nclusterExport(cl, c(\"nsim\", \"pisim\"))\n# Benchmarking: parSapply and sapply will run this simulation\n# a hundred times each, so at the end we have 1e5*100 points\n# to approximate pi\nmicrobenchmark::microbenchmark(\n  parallel = parSapply(cl, 1:100, pisim, nsim=nsim),\n  serial   = sapply(1:100, pisim, nsim=nsim),\n  times    = 1\n)\n\nUnit: milliseconds\n     expr      min       lq     mean   median       uq      max neval\n parallel 295.0158 295.0158 295.0158 295.0158 295.0158 295.0158     1\n   serial 405.3509 405.3509 405.3509 405.3509 405.3509 405.3509     1\n\n\n\n\nans_par &lt;- parSapply(cl, 1:100, pisim, nsim=nsim)\nans_ser &lt;- sapply(1:100, pisim, nsim=nsim)\nstopCluster(cl)\n\n\n\n     par      ser        R \n3.141762 3.141266 3.141593"
  },
  {
    "objectID": "rcpp-part1.html#see-also",
    "href": "rcpp-part1.html#see-also",
    "title": "5  Rcpp",
    "section": "5.13 See also",
    "text": "5.13 See also\n\nPackage parallel\nUsing the iterators package\nUsing the foreach package\n32 OpenMP traps for C++ developers\nThe OpenMP API specification for parallel programming\n‘openmp’ tag in Rcpp gallery\nOpenMP tutorials and articles\n\nFor more, check out the CRAN Task View on HPC\n\n\n\n\nEddelbuettel, Dirk. 2013. Seamless R and C++ Integration with Rcpp. New York: Springer. https://doi.org/10.1007/978-1-4614-6868-4.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++: A Brief Introduction to extitRcpp.” The American Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08."
  },
  {
    "objectID": "misc.html#general-resources",
    "href": "misc.html#general-resources",
    "title": "6  Misc",
    "section": "6.1 General resources",
    "text": "6.1 General resources\nThe Center for Advanced Research Computing (formerly HPCC) has tons of resources online. Here are a couple of useful links:\n\nCenter for Advanced Research Computing Website https://carc.usc.edu\nUser forum (very useful!) https://hpc-discourse.usc.edu/categories\nMonitor your account https://hpcaccount.usc.edu/\nSlurm Jobs Templates https://carc.usc.edu/user-information/user-guides/high-performance-computing/slurm-templates\nUsing R https://carc.usc.edu/user-information/user-guides/software-and-programming/r"
  },
  {
    "objectID": "misc.html#data-pointers",
    "href": "misc.html#data-pointers",
    "title": "6  Misc",
    "section": "6.2 Data Pointers",
    "text": "6.2 Data Pointers\nIMHO, these are the most important things to know about data management at USC’s HPC:\n\nDo your data transfer using the transfer nodes (it is faster).\nNever use your home directory as a storage space (use your project’s allotted space instead).\nUse the scratch filesystem for temp data only, i.e., never save important files in scratch.\nFinally, besides of Secure copy protocol (scp), if you are like me, try setting up a GUI client for moving your data (see this)."
  },
  {
    "objectID": "misc.html#the-slurm-options-they-forgot-to-tell-you-about",
    "href": "misc.html#the-slurm-options-they-forgot-to-tell-you-about",
    "title": "6  Misc",
    "section": "6.3 The Slurm options they forgot to tell you about…",
    "text": "6.3 The Slurm options they forgot to tell you about…\nFirst of all, you have to be aware that the only thing Slurm does is allocate resources. If your application uses parallel computing or not, that’s another story.\nHere some options that you need to be aware of:\n\nntasks (default 1) This tells Slurm how many processes you will have running. Notice that processes need not to be in the same node (so Slurm may reserve space in multiple nodes)\ncpus-per-task (defatult 1) This is how many CPUs each task will be using. This is what you need to use if you are using OpenMP (or a package that uses that), or anything you need to keep within the same node.\nnodes the number of nodes you want to use in your job. This is useful mostly if you care about the maximum (I would say) number of nodes you want your job to work. So, for example, if you want to use 8 cores for a single task and force it to be in the same node, you would add the option --nodes=1/1.\nmem-per-cpu (default 1GB) This is the MINIMUM amount of memory you want Slurm to allocate for the task. Not a hard barrier, so your process can go above that.\ntime (default 30min) This is a hard limit as well, so if you job takes more than the specified time, Slurm will kill it.\npartition (default ““) and account (default”“) these two options go along together, this tells Slurm what resources to use. Besides of the private resources we have the following:\n\nquick partition: Any job that is small enough (in terms of time and memory) will go this way. This is usually the default if you don’t specify any memory or time options.\nmain partition: Jobs that require more resources will go in this line.\nscavenge partition: If you need a massive number resources, and have a job that shouldn’t, in principle, take too long to finalize (less than a couple of hours), and you are OK with someone killing it, then this queue is for you. The Scavenge partition uses all the idle resources of the private partitions, so if any of the owners requests the resources, Slurm will cancel your job, i.e. you have no priority (see more).\nlargemem partition: If you need lots of memory, we have 4 1TB nodes for that.\n\nMore information about the partitions here"
  },
  {
    "objectID": "misc.html#good-practices-recomendations",
    "href": "misc.html#good-practices-recomendations",
    "title": "6  Misc",
    "section": "6.4 Good practices (recomendations)",
    "text": "6.4 Good practices (recomendations)\nThis is what you should use as a minimum:\n#SBATCH --output=simulation.out\n#SBATCH --job-name=simulation\n#SBATCH --time=04:00:00\n#SBATCH --mail-user=[you]@usc.edu\n#SBATCH --mail-type=END,FAIL\n\noutput is the name of the logfile to which Slurm will write.\njob-name is that, the name of the job. You can use this to either kill or at least be able to identify what is what you are running when you use myqueue\ntime Try always to set a time estimate (plus a little more) for your job.\nmail-user, mail-type so Slurm notifies you when things happen\n\nAlso, in your R code\n\nAny I/O should be done to either Scratch (/scratch/[your usc net id]) or Tmp Sys.getenv(\"TMPDIR\")."
  },
  {
    "objectID": "misc.html#running-r-interactively",
    "href": "misc.html#running-r-interactively",
    "title": "6  Misc",
    "section": "6.5 Running R interactively",
    "text": "6.5 Running R interactively\n\nThe HPC has several pre-installed pieces of software. R is one of those.\nTo access the pre-installed software, we use the Lmod module system (more information here)\nIt has multiple versions of R installed. Use your favorite one by running\nmodule load R/4.2.2/[version number]\nWhere [version number] can be 3.5.6 and up to 4.0.3 (the latest update). The usc module automatically loads gcc/8.3.0, openblas/0.3.8, openmpi/4.0.2, and pmix/3.1.3.\nIt is never a good idea to use your home directory to install R packages, that’s why you should try using a symbolic link instead, like this\ncd ~\nmkdir -p /path/to/a/project/with/lots/of/space/R\nln -s /path/to/a/project/with/lots/of/space/R R\nThis way, whenever you install your R packages, R will default to that location\nYou can run interactive sessions on HPC, but this recommended to be done using the salloc function in Slurm, in other words, NEVER EVER USE R (OR ANY SOFTWARE) TO DO DATA ANALYSIS IN THE HEAD NODES! The options passed to salloc are the same options that can be passed to sbatch (see the next section.) For example, if need to do some analyses in the thomas partition (which is private and I have access to), I would type something like\nsalloc --account=lc_pdt --partition=thomas --time=02:00:00 --mem-per-cpu=2G\nThis would put me in a single node allocating 2 gigs of memory for a maximum of 2 hours."
  },
  {
    "objectID": "misc.html#nonos-when-using-r",
    "href": "misc.html#nonos-when-using-r",
    "title": "6  Misc",
    "section": "6.6 NoNos when using R",
    "text": "6.6 NoNos when using R\n\nDo computation on the head node (compile stuff is OK)\nRequest a number of nodes (unless you know what you are doing)\nUse your home directory for I/O\nSave important information in Staging/Scratch"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dowle, Matt, and Arun Srinivasan. 2021. Data.table: Extension of\n‘Data.frame‘. https://CRAN.R-project.org/package=data.table.\n\n\nEddelbuettel, Dirk. 2013. Seamless R and\nC++ Integration with Rcpp. New York:\nSpringer. https://doi.org/10.1007/978-1-4614-6868-4.\n\n\nEddelbuettel, Dirk, and James Joseph Balamuta. 2018. “Extending extitR with extitC++:\nA Brief Introduction to extitRcpp.” The\nAmerican Statistician 72 (1): 28–36. https://doi.org/10.1080/00031305.2017.1375990.\n\n\nEddelbuettel, Dirk, and Romain François. 2011. “Rcpp:\nSeamless R and C++ Integration.”\nJournal of Statistical Software 40 (8): 1–18. https://doi.org/10.18637/jss.v040.i08.\n\n\nLUMI consortium. 2023. “Documentation - Distribution and\nBinding.” https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/distribution-binding/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nVega Yon, George, and Paul Marjoram. 2019. “slurmR: A Lightweight\nWrapper for HPC with Slurm.” The Journal of Open Source\nSoftware 4 (39). https://doi.org/10.21105/joss.01493.\n\n\n———. 2022. slurmR: A Lightweight Wrapper for\n’Slurm’. https://github.com/USCbiostats/slurmR.\n\n\nYoo, Andy B., Morris A. Jette, and Mark Grondona. 2003. “SLURM:\nSimple Linux Utility for Resource Management.” In Job\nScheduling Strategies for Parallel Processing, edited by Dror\nFeitelson, Larry Rudolph, and Uwe Schwiegelshohn, 2862:44–60. Lecture\nNotes in Computer Science. Berlin, Heidelberg: Springer Berlin\nHeidelberg. https://doi.org/10.1007/10968987_3."
  }
]